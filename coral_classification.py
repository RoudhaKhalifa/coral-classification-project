# -*- coding: utf-8 -*-
"""cvproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IQXT2d19qhSZSIFGDmZfFO0NiCbb1hPt
"""

from tqdm import tqdm

#imports
import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms, models
from torch.utils.data import DataLoader
from collections import Counter
from tqdm import tqdm
import matplotlib.pyplot as plt

# device setup (GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Running on:", device)

# download dataset from kaggle
import kagglehub
path = kagglehub.dataset_download("aneeshdighe/corals-classification")
print("Dataset downloaded to:", path)

# inspect directory structure
!ls "$path"

# CBAM-ResNet18 Architecture (Channel + Spatial Attention)

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchvision import models

# Channel Attention ---
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=16):
        super(ChannelAttention, self).__init__()
        self.sharedMLP = nn.Sequential(
            nn.Linear(in_planes, in_planes // ratio, bias=False),
            nn.ReLU(),
            nn.Linear(in_planes // ratio, in_planes, bias=False)
        )
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.sharedMLP(self.avg_pool(x).view(x.size(0), -1))
        max_out = self.sharedMLP(self.max_pool(x).view(x.size(0), -1))
        out = avg_out + max_out
        return self.sigmoid(out).unsqueeze(2).unsqueeze(3)


# Spatial Attention ---
class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()
        padding = (kernel_size - 1) // 2
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x_cat = torch.cat([avg_out, max_out], dim=1)
        return self.sigmoid(self.conv1(x_cat))


# CBAM Basic Block ---
class BasicBlockCBAM(nn.Module):
    def __init__(self, in_planes, planes, stride=1, downsample=None):
        super(BasicBlockCBAM, self).__init__()
        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3,
                               stride=stride, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(planes)
        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,
                               stride=1, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(planes)
        self.ca = ChannelAttention(planes)
        self.sa = SpatialAttention()
        self.relu = nn.ReLU(inplace=True)
        self.downsample = downsample

    def forward(self, x):
        residual = x
        out = self.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out = self.ca(out) * out
        out = self.sa(out) * out
        if self.downsample is not None:
            residual = self.downsample(x)
        out += residual
        out = self.relu(out)
        return out


# ResNet18 + CBAM Integration ---
class ResNetCBAM(nn.Module):
    def __init__(self, num_classes=2):
        super(ResNetCBAM, self).__init__()
        self.backbone = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)

        # Replace layer4 with CBAM-enhanced residual blocks
        self.backbone.layer4 = nn.Sequential(
            BasicBlockCBAM(256, 512, stride=2, downsample=nn.Sequential(
                nn.Conv2d(256, 512, kernel_size=1, stride=2, bias=False),
                nn.BatchNorm2d(512)
            )),
            BasicBlockCBAM(512, 512)
        )

        in_features = self.backbone.fc.in_features
        self.backbone.fc = nn.Linear(in_features, num_classes)

    def forward(self, x):
        return self.backbone(x)

# Dataset & DataLoader Setup (Training + Testing)

from torchvision import datasets, transforms
from torch.utils.data import DataLoader
import cv2, numpy as np
from PIL import Image

# --- CLAHE preprocessing for coral images ---
def apply_clahe(pil_img):
    img = np.array(pil_img)
    img_lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)
    l, a, b = cv2.split(img_lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    cl = clahe.apply(l)
    merged = cv2.merge((cl, a, b))
    img_clahe = cv2.cvtColor(merged, cv2.COLOR_LAB2RGB)
    return Image.fromarray(img_clahe)

# --- Data augmentation & normalization ---
train_transform = transforms.Compose([
    transforms.Lambda(lambda img: apply_clahe(img)),
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomVerticalFlip(p=0.3),
    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.2, hue=0.05),
    transforms.RandomRotation(15),
    transforms.ToTensor(),
])

test_transform = transforms.Compose([
    transforms.Lambda(lambda img: apply_clahe(img)),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

# --- Dataset paths ---
train_dir = "/root/.cache/kagglehub/datasets/aneeshdighe/corals-classification/versions/1/Bleached Corals and Healthy Corals Classification/Training"
test_dir = "/root/.cache/kagglehub/datasets/aneeshdighe/corals-classification/versions/1/Bleached Corals and Healthy Corals Classification/Testing"

# --- Load datasets ---
train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)
test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)

# --- Create loaders ---
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

print(f"âœ… Dataset ready: {len(train_dataset)} train samples, {len(test_dataset)} test samples.")
print(f"Classes: {train_dataset.classes}")

# training setup

num_classes = 2  # Healthy vs Bleached
model = ResNetCBAM(num_classes).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
epochs = 5

#  Grad-CAM extractor for trained CBAM-ResNet18
from torchcam.methods import GradCAM

# Connect Grad-CAM to the last convolutional block of ResNet18
cam_extractor = GradCAM(model, target_layer="backbone.layer4")

print("âœ… Grad-CAM extractor initialized successfully.")

# Grad-CAM Comparison: Healthy vs Bleached Coral
import cv2
import numpy as np
import matplotlib.pyplot as plt
from PIL import Image
import torch
from torchvision import transforms

# Make sure model and CAM extractor are ready
model.eval()

import os, random

# --- Automatically pick random, valid images from your test folders ---
healthy_dir = "/root/.cache/kagglehub/datasets/aneeshdighe/corals-classification/versions/1/Bleached Corals and Healthy Corals Classification/Testing/healthy_corals"
bleached_dir = "/root/.cache/kagglehub/datasets/aneeshdighe/corals-classification/versions/1/Bleached Corals and Healthy Corals Classification/Testing/bleached_corals"

healthy_img_path = os.path.join(healthy_dir, random.choice(os.listdir(healthy_dir)))
bleached_img_path = os.path.join(bleached_dir, random.choice(os.listdir(bleached_dir)))

print("âœ… Selected Healthy Image:", healthy_img_path)
print("âœ… Selected Bleached Image:", bleached_img_path)

# --- Define preprocessing (same as training) ---
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
])

def generate_gradcam(img_path, label_name):
    img = Image.open(img_path).convert("RGB")
    input_tensor = transform(img).unsqueeze(0).to(device)

    # Forward pass
    output = model(input_tensor)
    pred_class = output.argmax(dim=1).item()

    # Grad-CAM extraction (retain computation graph)
    activation_map = cam_extractor(pred_class, output, retain_graph=True)[0]
    activation_map = torch.nn.functional.interpolate(
        activation_map.unsqueeze(1),
        size=(img.size[1], img.size[0]),
        mode='bilinear',
        align_corners=False
    )[0, 0].detach().cpu().numpy()

    # Normalize heatmap for visibility
    activation_map = (activation_map - activation_map.min()) / (activation_map.max() - activation_map.min())

    # Create vivid overlay
    img_cv = np.array(img)
    img_cv = cv2.cvtColor(img_cv, cv2.COLOR_RGB2BGR)
    heatmap = cv2.applyColorMap(np.uint8(255 * activation_map), cv2.COLORMAP_JET)
    overlay = cv2.addWeighted(heatmap, 0.6, img_cv, 0.4, 0)
    overlay = cv2.cvtColor(overlay, cv2.COLOR_BGR2RGB)

    return img, overlay, pred_class, label_name

# Load your final trained model (with CLAHE + CBAM)
model.load_state_dict(torch.load("resnet18_cbam_coral_clahe.pth"))
model.eval()

# --- Generate both Grad-CAMs ---
healthy_img, healthy_overlay, pred_h, _ = generate_gradcam(healthy_img_path, "Healthy Coral")
bleached_img, bleached_overlay, pred_b, _ = generate_gradcam(bleached_img_path, "Bleached Coral")

# --- Display results ---
plt.figure(figsize=(14, 6))

plt.subplot(2, 2, 1)
plt.imshow(healthy_img)
plt.title("Original â€” Healthy Coral")
plt.axis('off')

plt.subplot(2, 2, 2)
plt.imshow(healthy_overlay)
plt.title(f"Grad-CAM â€” Predicted: {'Healthy' if pred_h==1 else 'Bleached'} Coral")
plt.axis('off')

plt.subplot(2, 2, 3)
plt.imshow(bleached_img)
plt.title("Original â€” Bleached Coral")
plt.axis('off')

plt.subplot(2, 2, 4)
plt.imshow(bleached_overlay)
plt.title(f"Grad-CAM â€” Predicted: {'Healthy' if pred_b==1 else 'Bleached'} Coral")
plt.axis('off')

plt.tight_layout()
plt.show()

import torch
import torch.nn as nn
import torch.optim as optim
from tqdm import tqdm

# --- Device setup ---
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("âœ… Using device:", device)

# --- Define loss and optimizer ---
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-4)
epochs = 5  # You can increase to 10 for better results

best_acc = 0.0

for epoch in range(epochs):
    model.train()
    running_loss, correct, total = 0.0, 0, 0

    # --- Training loop ---
    for imgs, labels in tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}"):
        imgs, labels = imgs.to(device), labels.to(device)

        optimizer.zero_grad()
        outputs = model(imgs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    train_acc = 100.0 * correct / total
    train_loss = running_loss / len(train_loader)

    # --- Validation loop ---
    model.eval()
    val_correct, val_total, val_loss = 0, 0, 0.0
    with torch.no_grad():
        for imgs, labels in test_loader:
            imgs, labels = imgs.to(device), labels.to(device)
            outputs = model(imgs)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = outputs.max(1)
            val_total += labels.size(0)
            val_correct += predicted.eq(labels).sum().item()

    val_acc = 100.0 * val_correct / val_total
    val_loss /= len(test_loader)

    print(f"Epoch [{epoch+1}/{epochs}] "
          f"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}% "
          f"| Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%")

    # --- Save best model ---
    if val_acc > best_acc:
        best_acc = val_acc
        torch.save(model.state_dict(), "resnet18_cbam_coral_clahe.pth")
        print(f"âœ… Best model saved with {best_acc:.2f}% validation accuracy")

print("ðŸŽ¯ Training complete! Best validation accuracy:", best_acc)

from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
import torch

# Load the best model
model.load_state_dict(torch.load("resnet18_cbam_coral_clahe.pth"))
model.eval()

y_true, y_pred = [], []

with torch.no_grad():
    for imgs, labels in test_loader:
        imgs, labels = imgs.to(device), labels.to(device)
        outputs = model(imgs)
        _, predicted = torch.max(outputs, 1)
        y_true.extend(labels.cpu().numpy())
        y_pred.extend(predicted.cpu().numpy())

# Classification report
print("ðŸ“Š Classification Report:\n", classification_report(y_true, y_pred, target_names=test_dataset.classes))

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_dataset.classes)
disp.plot(cmap="Blues")
plt.title("Confusion Matrix â€” Healthy vs Bleached Corals")
plt.show()

model.load_state_dict(torch.load("resnet18_cbam_coral_clahe.pth"))